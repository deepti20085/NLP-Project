# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UZGyAm35KV4aSgirDSJkrbIhDIlS4tEI
"""

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier, XGBRegressor
from lightgbm import LGBMClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
!pip install contractions
!pip install nltk==3.5
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, KFold
# import pickle
import re
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from html.parser import HTMLParser
import contractions
from nltk.tokenize import word_tokenize
import nltk
from nltk.corpus import stopwords
from textblob import TextBlob
import sys
from nltk.stem import SnowballStemmer
nltk.download('stopwords')
nltk.downloader.download('vader_lexicon')
stopwords_nltk = stopwords.words('english')
snowball_stemmer = SnowballStemmer('english')
import re
nltk.download('punkt')
from nltk.corpus import stopwords   
import re  
import numpy as np
from bs4 import BeautifulSoup
stop_words = set(stopwords.words('english'))
from nltk.stem import PorterStemmer
lancaster=PorterStemmer()
from nltk.corpus import wordnet
nltk.downloader.download('wordnet')

import codecs
import string
import os
import re
from nltk.corpus import wordnet
import contractions
import nltk
import pandas as pd
from nltk.tokenize import sent_tokenize, word_tokenize
from collections import OrderedDict, defaultdict, Counter
from nltk.tag import pos_tag
from sklearn.svm import SVR
import pickle
import nltk.util 
from nltk.util import *
from nltk.corpus import stopwords

"""DATA EXPLORATION AND VISUALIZATION"""

#sakshi code
!pip install contractions
!pip install beautifulSoup4
!pip install lexical-diversity
import nltk
nltk.download('stopwords')
nltk.download('punkt')

train_df = pd.read_csv("/content/drive/My Drive/NLP Project/train.csv")
train_df.dtypes
from wordcloud import WordCloud 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from wordcloud import WordCloud

train_df = pd.read_csv("/content/drive/My Drive/NLP Project/train.csv")
def ngrams(data,n):
    words = nltk.word_tokenize(text)
    ngram = list(nltk.ngrams(words,n))
    return ngram
def joinbigram(bigram):
  for i in range(0,len(bigram)):
      bigram[i] ="_".join(bigram[i])
  freq_bigram = nltk.FreqDist(bigram)
  return freq_bigram
def plotWordCloud(freq_ngram,f):
  if (f==1):
    wordcloud = WordCloud(random_state = 21).generate_from_frequencies(freq_ngram)
  elif (f==0):
    wordcloud = WordCloud(random_state=21).generate(freq_ngram)
  plt.figure(figsize = (15,10))
  plt.imshow(wordcloud,interpolation = 'bilinear')
  plt.axis("off")
  plt.show()
#unigram wordcloud
text=" ".join([str(d) for d in train_df['text']])
plotWordCloud(text,0)

#bigram wordcloud
text=" ".join(str(d) for d in train_df.text)
bigram=ngrams(text,2)
freq_bigram=joinbigram(bigram)
plotWordCloud(freq_bigram,1)

"""FAKE NEWS"""

#fake unigram wordcloud
text = " ".join([str(d) for d in train_df.text[train_df.label==1]])
plotWordCloud(text,0)

#fake news bigram wordcloud
text = " ".join([str(d) for d in train_df.text[train_df.label==1]])
bigram_real=ngrams(text,2)
freq_bigram_real=joinbigram(bigram_real)
plotWordCloud(freq_bigram_real,1)

"""REAL NEWS """

#real news unigram wordcloud
text = " ".join([d for d in train_df.text[train_df.label==0]])
plotWordCloud(text,0)

#real news bigram wordcloud
text = " ".join([d for d in train_df.text[train_df.label==0]])
bigram_fake=ngrams(text,2)
freq_bigram_fake=joinbigram(bigram_fake)
plotWordCloud(freq_bigram_fake,1)

"""DATA CHECK"""

#label count to check for data imbalance.
print("fake news label count:",train_df.text[train_df["label"]==0].count())
print("real news label count:",train_df.text[train_df["label"]==1].count())
labels=train_df["label"].value_counts()
sns.barplot(labels.index, labels)
plt.title('Label count', fontsize=12)

#characters in text of fake news and real news.
fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)
length=train_df[train_df["label"]==1]['text'].str.len()
ax1.hist(length,bins = 20,color='red')
ax1.set_title('Fake News')
length1=train_df[train_df["label"]==0]['text'].str.len()
ax2.hist(length1, bins = 20,color='blue')
ax2.set_title('Real News')
fig.suptitle('Characters in text')
plt.show()

from nltk import tokenize
def frequentWordCounter(text,num,color):
    df=pd.DataFrame()
    text=' '.join([str(t) for t in text])
    spacer=tokenize.SpaceTokenizer()
    tokens=spacer.tokenize(text)
    freq_words=nltk.FreqDist(tokens)
    words=list(freq_words.keys())
    freq=list(freq_words.values())
    df['word']=words
    df['freq']=freq
    df=df.nlargest(columns='freq',n=num)
    plt.figure(figsize=(12,8))
    ax = sns.barplot(data=df,x ='word',y='freq',color =color)
    ax.set(ylabel="count")
    plt.xticks(rotation='vertical')
    plt.show()

#most frequent 50 words in fake news
frequentWordCounter(train_df.text[train_df["label"] == 1],50,'red')

#most frequent 50 words in real news.
frequentWordCounter(train_df.text[train_df["label"]==0],50,'orange')

"""START HERE"""



train_df = pd.read_csv("/content/drive/My Drive/NLP Project/train.csv")
test_df = pd.read_csv("/content/drive/My Drive/NLP Project/test.csv")

train_df.head()

ftrain = pd.read_csv("/content/drive/My Drive/NLP Project/featured_train.csv")
ftest = pd.read_csv("/content/drive/My Drive/NLP Project/featured_test.csv")
features=pd.read_csv("/content/drive/MyDrive/NLP Project/sakshi_features(29.11.2020).csv")
featuresTest= pd.read_csv("/content/drive/MyDrive/NLP Project/sakshi_features_test(29.11.2020).csv")

def CountStats(sentence):
    print("exclamation marks:", len(re.findall(r"(\!)\1+", sentence)))
    print("muktiple question marks:", len(re.findall(r"(\?)\1+", sentence)))
    print("repetition of stop words",len(re.findall(r"(\.)\1+", sentence)))
    regex = re.compile(r"(.)\1{2}")
    print("elongsted word count :",len([word for word in sentence.split() if regex.search(word)]) )
    c = len(re.findall(':\)|;\)|:-\)|\(-:|:-D|=D|:P|xD|X-p|\^\^|:-*|\^\.\^|\^\-\^|\^\_\^|\,-\)|\)-:|:\'\(|:\(|:-\(|:\S|T\.T|\.\_\.|:<|:-\S|:-<|\*\-\*|:O|=O|=\-O|O\.o|XO|O\_O|:-\@|=/|:/|X\-\(|>\.<|>=\(|D:', sentence))
    print("emoticons count",c)
       
def NegateTags(sentence):
        l=['no', 'never', 'not']
        #sentence=word_tokenize(sentence) 
        sent = []
        s=0
        for word in sentence:
            if word in l:
                s=s+1
                sent.append(word)
                sent.append(" ")
            else:
                sent.append(word)
                sent.append(" ")
        sent = sent[:-1]
        print("not tagged : ", s)
        print(sent)
        # return  s
    
def CapitaliseTags(sentence):
        """ Finds a word with at least 3 characters capitalized and adds the tag ALL_CAPS_ """
        sentence=word_tokenize(sentence) 
        sent = []
        s=0
        for word in sentence:
            if(len(re.findall("[A-Z]{3,}", word))):
                s=s+1
                sent.append(word)
                sent.append(" ")
            else:
                sent.append(word)
                sent.append(" ")
        sent = sent[:-1]
        print("count of word with at least 3 characters capitalized : ", s)
        print(sent)
        # return  s

def stemSentence(sentence, snowball_stemmer):
        token_words=word_tokenize(str(sentence))
        stem_sentence=[]
        for word in token_words:
            stem_sentence.append(snowball_stemmer.stem(word))
            stem_sentence.append(" ")
        return ("".join(stem_sentence))
# d = stemSentence(cleaned_text,snowball_stemmer)
train = train_df['text'].astype(str)
print("Before feature Vectorization:Train:  ")
precleaned_text=[]
for k in train:
   precleaned_text.append(k)
sentence= precleaned_text
CountStats(str(sentence))  

test = test_df['text'].astype(str)
print()
print("Before feature Vectorization:Test: ")
precleaned_textTest=[]
for k in test:
   precleaned_textTest.append(k)
sentence= precleaned_textTest
CountStats(str(sentence))

contraction_mapping = {"ain't": "is not", "aren't": "are not","can't": "cannot", "'cause": "because", "could've": "could have", "couldn't": "could not",

                           "didn't": "did not", "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not",

                           "he'd": "he would","he'll": "he will", "he's": "he is", "how'd": "how did", "how'd'y": "how do you", "how'll": "how will", "how's": "how is",

                           "I'd": "I would", "I'd've": "I would have", "I'll": "I will", "I'll've": "I will have","I'm": "I am", "I've": "I have", "i'd": "i would",

                           "i'd've": "i would have", "i'll": "i will",  "i'll've": "i will have","i'm": "i am", "i've": "i have", "isn't": "is not", "it'd": "it would",

                           "it'd've": "it would have", "it'll": "it will", "it'll've": "it will have","it's": "it is", "let's": "let us", "ma'am": "madam",

                           "mayn't": "may not", "might've": "might have","mightn't": "might not","mightn't've": "might not have", "must've": "must have",

                           "mustn't": "must not", "mustn't've": "must not have", "needn't": "need not", "needn't've": "need not have","o'clock": "of the clock",

                           "oughtn't": "ought not", "oughtn't've": "ought not have", "shan't": "shall not", "sha'n't": "shall not", "shan't've": "shall not have",

                           "she'd": "she would", "she'd've": "she would have", "she'll": "she will", "she'll've": "she will have", "she's": "she is",

                           "should've": "should have", "shouldn't": "should not", "shouldn't've": "should not have", "so've": "so have","so's": "so as",

                           "this's": "this is","that'd": "that would", "that'd've": "that would have", "that's": "that is", "there'd": "there would",

                           "there'd've": "there would have", "there's": "there is", "here's": "here is","they'd": "they would", "they'd've": "they would have",

                           "they'll": "they will", "they'll've": "they will have", "they're": "they are", "they've": "they have", "to've": "to have",

                           "wasn't": "was not", "we'd": "we would", "we'd've": "we would have", "we'll": "we will", "we'll've": "we will have", "we're": "we are",

                           "we've": "we have", "weren't": "were not", "what'll": "what will", "what'll've": "what will have", "what're": "what are",

                           "what's": "what is", "what've": "what have", "when's": "when is", "when've": "when have", "where'd": "where did", "where's": "where is",

                           "where've": "where have", "who'll": "who will", "who'll've": "who will have", "who's": "who is", "who've": "who have",

                           "why's": "why is", "why've": "why have", "will've": "will have", "won't": "will not", "won't've": "will not have",

                           "would've": "would have", "wouldn't": "would not", "wouldn't've": "would not have", "y'all": "you all",

                           "y'all'd": "you all would","y'all'd've": "you all would have","y'all're": "you all are","y'all've": "you all have",

                           "you'd": "you would", "you'd've": "you would have", "you'll": "you will", "you'll've": "you will have",

                           "you're": "you are", "you've": "you have","i'ma":"i am going to"}

html_parser=HTMLParser()
def posttext_cleaner(text):
    newString= text.lower()
    newString = re.sub("\S*\d\S*", "", newString).strip()
    newString = re.sub(r"@(\s+|\w+)","",newString)  #users eliminate
    newString = contractions.fix(newString)   #expand
    newString = re.sub('[^a-zA-Z0-9!\?\s.,]', '', newString)  #to eliminate non ascii character
    newString = re.sub(r"\s{2,}[\.|,]+\s{2,}","",newString)
    newString = re.sub(r"[\.,]{2,}"," ",newString)
    newString = BeautifulSoup(newString, "lxml").text
    newString = BeautifulSoup(text,"html.parser",from_encoding='utf_8').get_text()
    newString = re.sub(r'\([^)]*\)', '', newString)
    newString = re.sub('"','', newString)
    newString = re.sub(r"http\S+", "", newString)
    newString = re.sub(r":\)|;\)|:-\)|\(-:|:-D|=D|:P|xD|X-p|\^\^|:-*|\^\.\^|\^\-\^|\^\_\^|\,-\)|\)-:|:\'\(|:\(|:-\(|:\S|T\.T|\.\_\.|:<|:-\S|:-<|\*\-\*|:O|=O|=\-O|O\.o|XO|O\_O|:-\@|=/|:/|X\-\(|>\.<|>=\(|D:", "", newString)
    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(" ")])    
    newString = re.sub(r"'s\b","",newString)
    newString  = re.sub('[^A-Za-z0-9]+', ' ', newString)
    tokens = [w for w in newString.split() if not w in stop_words]
    long_words=[]
    for i in tokens:
        if len(i)>0:                  #removing short word
            long_words.append(i)   
    return (" ".join(long_words)).strip()

postcleaned_textTest=[]
for k in test:
   postcleaned_textTest.append(posttext_cleaner(k))

postcleaned_text=[]
for k in train:
   postcleaned_text.append(posttext_cleaner(k))

print(CountStats(str(postcleaned_text)))
print(CountStats(str(postcleaned_textTest)))

clean_data = pd.DataFrame(postcleaned_text)
clean_data_test = pd.DataFrame(postcleaned_textTest)
data = train_df.copy()
ndata = test_df.copy()
frames = [data,clean_data]
newData = pd.concat(frames,axis=1)
newData.head()
newData=newData.fillna(0)
newData['label'] = newData['label'].astype(int)
newData.drop('id',axis=1,inplace=True)
newData.columns=['title','author','preCleantext','label','postCleanText']
newData.drop('title',axis=1,inplace=True)
newData.drop('author',axis=1,inplace=True)
newData.head()
frames = [ndata,clean_data_test]
newDatat = pd.concat(frames,axis=1)
newDatat.head()
newDatat=newDatat.fillna(0)
newDatat.drop('id',axis=1,inplace=True)
newDatat.columns=['title','author','preCleantext','postCleanText']
newDatat.head()
newDatat.drop('title',axis=1,inplace=True)
newDatat.drop('author',axis=1,inplace=True)
newDatat.head()

!pip install afinn
from afinn import Afinn
afinn = Afinn()
afinn = Afinn(emoticons=True)
for item in newDatat['preCleantext']:
    # print(item)
    newDatat['AFFIN_SCORE']=afinn.score(str (item))
for item in newData['preCleantext']:
    # print(item)
    newData['AFFIN_SCORE']=afinn.score(str (item))

print("affin done")

# def exclamation(sentence):
#    return len(re.findall(r"(\!)\1+", str(sentence)))

# def questnmarks(sentence):
#    return  len(re.findall(r"(\?)\1+", str(sentence)))

# def rep_stopwords(sentence):
#     return len(re.findall(r"(\.)\1+", str(sentence)))

# def elongated_word(sentence):
#     regex = re.compile(r"(.)\1{2}")
#     return len([word for word in str(sentence).split() if regex.search(word)]) 

# def emoticon_count(sentence):
#      return len(re.findall(':\)|;\)|:-\)|\(-:|:-D|=D|:P|xD|X-p|\^\^|:-*|\^\.\^|\^\-\^|\^\_\^|\,-\)|\)-:|:\'\(|:\(|:-\(|:\S|T\.T|\.\_\.|:<|:-\S|:-<|\*\-\*|:O|=O|=\-O|O\.o|XO|O\_O|:-\@|=/|:/|X\-\(|>\.<|>=\(|D:', str(sentence)))

# def NegateTags(sentence):
#         l=['no', 'never', 'not']
#         sent = []
#         s=0
#         for word in str(sentence):
#             if word in l:
#                 s=s+1
#                 sent.append(word)
#                 sent.append(" ")
#             else:
#                 sent.append(word)
#                 sent.append(" ")
#         sent = sent[:-1]
#         return s

# def CapitaliseTags(sentence):
#         sentence=word_tokenize(str(sentence)) 
#         sent = []
#         s=0
#         for word in sentence:
#             if(len(re.findall("[A-Z]{3,}", word))):
#                 s=s+1
#                 sent.append(word)
#                 sent.append(" ")
#             else:
#                 sent.append(word)
#                 sent.append(" ")
#         sent = sent[:-1]
#         return s

# def stemSentence(sentence, snowball_stemmer):
#         token_words=word_tokenize(str(sentence))
#         stem_sentence=[]
#         for word in token_words:
#             stem_sentence.append(snowball_stemmer.stem(word))
#             stem_sentence.append(" ")
#         return ("".join(stem_sentence))

# newDatat['exclamationCount']  = test_df['text'].apply(exclamation)
# newDatat['questnmarkCount'] = test_df['text'].apply(questnmarks)
# newDatat['rep_stopwordsCount'] = test_df['text'].apply(rep_stopwords)
# newDatat['elongatedwordCount'] = test_df['text'].apply(elongated_word)
# newDatat['emoticon_count'] = test_df['text'].apply(emoticon_count)
# newDatat['negation']=test_df['text'].apply(NegateTags)
# newDatat['caps'] = test_df['text'].apply(CapitaliseTags)
# newDatat['stems']=test_df['text'].apply(lambda row : stemSentence(row, snowball_stemmer))


# newData['exclamationCount']  = train_df['text'].apply(exclamation)
# newData['questnmarkCount'] = train_df['text'].apply(questnmarks)
# newData['rep_stopwordsCount'] = train_df['text'].apply(rep_stopwords)
# newData['elongatedwordCount'] = train_df['text'].apply(elongated_word)
# newData['emoticon_count'] = train_df['text'].apply(emoticon_count)
# newData['negation']=train_df['text'].apply(NegateTags)
# newData['caps'] = train_df['text'].apply(CapitaliseTags)
# newData['stems']=train_df['text'].apply(lambda row : stemSentence(row, snowball_stemmer))
# print(newData[:5])
# print(newDatat[:5])

# print("punctuation  donr")
# # # -----------------------------------------------------------------
# # FEATURE: MPQA
# def loadMPQALexicon():
#     negativeWords = {}
#     positiveWords = {}
#     with open('/content/drive/My Drive/Res/subjectivity_clues_hltemnlp05/subjectivity_clues_hltemnlp05/subjclueslen1-HLTEMNLP05.tff','r') as iFile:
#         for i in iFile:
#             # print(i)
#             row = i.rstrip().split(' ')
#             word = row[2].split('=')[1]
#             pol = row[5].split('=')[1]
#             if pol == 'positive':
#                 positiveWords[word] = 1
#             elif pol == 'negative':
#                 negativeWords[word] = 1
#     return {"pos":positiveWords, "neg":negativeWords}

# mpqa = loadMPQALexicon()
# #
# import re
# URL = re.compile('http://[^ ]+')
# EXCLAMATION = re.compile('!!+$')
# QUESTION = re.compile('\?\?+$')
# EXCLQUEST = re.compile('[?!][?!]+$')
# #USERNAMES = re.compile('(?<=^|(?<=[^a-zA-Z0-9-\.]))#([A-Za-z_]+[A-Za-z0-9_]+)')
# USERNAMES =  re.compile('(@\w+)+')
# punctuation = dict([(x,1) for x in '!.?,'])
# from nltk.tokenize import TweetTokenizer
# def genFeatures(text):
#    # text = text.lower()
#     newText  = USERNAMES.sub('{uname}', str(text)).replace('{uname}', '@TWITTERUSERNAME')
#     tt = TweetTokenizer()
#     tmp = tt.tokenize(newText)
#     # tmp = twokenize.tokenize(newText)
#     bigrams = dict([(tmp[x]+' '+tmp[x+1], 1) for x in range(len(tmp)-1) if EXCLQUEST.match(tmp[x]) is None and EXCLQUEST.match(tmp[x+1]) is None])
#     unigrams = dict([(tmp[x].lower(),1) for x in range(len(tmp)) if EXCLQUEST.match(tmp[x]) is None])
#     mpaaPosCount = len([ x for x in unigrams.keys() if x in mpqa['pos']])
#     mpaaNegCount = len([x for x in unigrams.keys() if x in mpqa['neg']])
#     extraFeatures = {"MPAAPOSCOUNT":mpaaPosCount, "MPAANEGCOUNT":mpaaNegCount}
#     finalDict = {}
#     for k,v in unigrams.items():
#         finalDict[k] = v
#     for k,v in bigrams.items():
#         finalDict[k] = v
#     for k,v in extraFeatures.items():
#         finalDict[k] = v
#     return finalDict

# X=[]


# newDatat['MPACOUNT']= newDatat['preCleantext'].apply(genFeatures)
# newDatat['MPAAPOSCOUNT']  =  newDatat['MPACOUNT'].apply(lambda traine : traine ['MPAAPOSCOUNT'])
# newDatat['MPAANEGCOUNT']  =  newDatat['MPACOUNT'].apply(lambda traine : traine ['MPAANEGCOUNT'])

# train_df['MPACOUNT']= train_df['text'].apply(genFeatures)
# train_df['MPAAPOSCOUNT']  = train_df['MPACOUNT'].apply(lambda traine : traine ['MPAAPOSCOUNT'])
# train_df['MPAANEGCOUNT']  =  train_df['MPACOUNT'].apply(lambda traine : traine ['MPAANEGCOUNT'])
# mpos=train_df['MPAAPOSCOUNT']
# mneg=train_df['MPAANEGCOUNT']
# newData['MPAANEGCOUNT']=mneg
# newData['MPAAPOSCOUNT']=mpos

# print("mpq done")

# nltk.download('stopwords')
# nltk.downloader.download('vader_lexicon')
# stopwords_nltk = stopwords.words('english')
# snowball_stemmer = SnowballStemmer('english')
# import re
# from nltk.sentiment.vader import SentimentIntensityAnalyzer
# sid = SentimentIntensityAnalyzer()
# newDatat['scores'] = newDatat['postCleanText'].apply(lambda text: sid.polarity_scores(text))
# newDatat['pos']  = newDatat['scores'].apply(lambda score_dict: score_dict['pos'])
# newDatat['neu']  = newDatat['scores'].apply(lambda score_dict: score_dict['neu'])
# newDatat['neg']  = newDatat['scores'].apply(lambda score_dict: score_dict['neg'])
# newDatat['compound']  = newDatat['scores'].apply(lambda score_dict: score_dict['compound'])

# newData['scores'] = newData['postCleanText'].apply(lambda text: sid.polarity_scores(text))
# newData['pos']  = newData['scores'].apply(lambda score_dict: score_dict['pos'])
# newData['neu']  = newData['scores'].apply(lambda score_dict: score_dict['neu'])
# newData['neg']  = newData['scores'].apply(lambda score_dict: score_dict['neg'])
# newData['compound']  = newData['scores'].apply(lambda score_dict: score_dict['compound'])

# def readFile(file):
#     x=codecs.open(file, 'r', encoding = 'utf-8', errors = 'ignore')
#     fileText=x.read()
#     #print("read the file")
#     return fileText

# def createDict(file,exp,wordDict):
#     Dict=[]
#     words=file.split('\n')
#     for word in words:
#         word=word.replace('\r','')
#         if(word!= ''):
#             Dict.append(word)
#             wordDict[word].append(exp)
#     return Dict ,wordDict    

# #created dictionary from BING lui lexicon
# wordDict=defaultdict(list)
# posDict = []
# negDict = []
# file1=readFile('/content/drive/My Drive/assignment3b/opinion-lexicon-English/negative-words.txt')
# file2=readFile('/content/drive/My Drive/assignment3b/opinion-lexicon-English/positive-words.txt')
# negDict,wordDict=createDict(file1,'negative',wordDict)
# posDict,wordDict=createDict(file2,'positive',wordDict)

# # feature 1 : positive and negative word count
# #feature1:Polar word count
# def polarWordCount(wordDict,tweets,features):
#     #print(tweets)
#     pList=[]
#     nList=[]
#     for tweet in tweets:
#         #print(tweet)
#         #print(tweet)
#         pcount=0.0
#         ncount=0.0
#         if not pd.isnull(tweet):
#           tokens=nltk.word_tokenize(tweet)
#           #print(tokens)
#           for token in tokens:
#               token=token.lower()
#               if token in wordDict:
#                   if (wordDict[token][0]=='positive'):
#                       pcount+=1
#                   elif (wordDict[token][0]=='negative'):
#                       ncount+=1    
#         pList.append(pcount)
#         nList.append(ncount)
#     features['binglui_pcount']=pList
#     features['binglui_ncount']=nList
#     return features

# def NRCDict(file):
#     PolarityScoreDict=defaultdict(list)
#     file4=readFile(file)
#     #print(file4)
#     lines=(file4.split('\n'))
#     lines=[line.split() for line in lines]
#     for i in range(0,len(lines)-1):
#         score=lines[i][1]
#         if float(score)>0:
#             polarity='positive'
#         elif float(score) < 0:
#             polarity='negative'
#         else:
#             polarity='neutral'
#         PolarityScoreDict[lines[i][0]].append(polarity)
#         PolarityScoreDict[lines[i][0]].append(score)
#     return PolarityScoreDict

# # feature2: aggregate polarity score
# def polarityScore(tweets,features):
#     pscore=[]
#     nscore=[]
#     p1score=[]
#     n1score=[]
#     p3score=[]
#     n3score=[]
#     Dict=NRCDict('/content/drive/My Drive/assignment3b/Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt')
#     for tweet in tweets:
#         p3_score=0.0
#         n3_score=0.0
#         if not pd.isnull(tweet):
#           tokens=nltk.word_tokenize(tweet)
#           for t in tokens:  
#             t=t.lower() 
#             if t in Dict: 
#               if (Dict[t][0]=='positive'):
#                   p3_score+=float(Dict[t][1])
#               elif (Dict[t][0]=='negative'):
#                   n3_score+=float(Dict[t][1])        
#         p3score.append(p3_score)
#         n3score.append(n3_score)
#     features['sent140_positive_score']= p3score
#     features['sent140_negative_score']= n3score
#     return features

# def emotionDict(e):
#     Dict=defaultdict(list)
#     file4=readFile('/content/drive/My Drive/assignment3b/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt')
#     lines=(file4.split('\n'))
#     lines=[line.split() for line in lines]
#     for i in range(0,len(lines)-1):
#         emotion=lines[i][1]
#         if(emotion==e):
#             val=lines[i][2]
#             Dict[lines[i][0]].append(val)
#     return Dict

# NRCwordDict = {}
# # emotionList = {}
# with open('/content/drive/My Drive/lexicons.zip (Unzipped Files)/lexicons/8. NRC-word-emotion-lexicon.txt', 'r') as f:
#     for row in f.readlines():
#         row = row.split()
#         if (row[1]=="positive" or row[1]=="negative"):
#             NRCwordDict[row[0]+"_"+row[1]] = float(row[2])

# print(NRCwordDict)

# def lexiconFeature(cleanTextList):
#   Nrc_tot_con_greaterthan0=[]
#   Nrc_tot_score=[]
#   Nrc_last_score=[]
#   for row in cleanTextList:
#       # print(row)
#       if not pd.isnull(row):
#           tokens=nltk.word_tokenize(row)
        
#       row = re.sub(r"[^a-zA-Z]$","",row) 
#       word_tokens=word_tokenize(row)
#       word_tokens=[word.lower() for word in word_tokens]
#       # print(word_tokens) 
#       count_great_0=0
#       total_score=0
#       last_soce=0
#       for word in word_tokens:
#         if(word+"_positive"  in NRCwordDict): 
#             sc=NRCwordDict.get(word+"_positive")
#             total_score=total_score+sc
#             if(sc>0):
#               count_great_0 = count_great_0 + 1
#         if(word+"_negative"  in NRCwordDict): 
#             sc=NRCwordDict.get(word+"_negative")
#             total_score=total_score+sc
#             if(sc>0):
#               count_great_0 = count_great_0 + 1       
#       Nrc_tot_con_greaterthan0.append(count_great_0)
#       Nrc_tot_score.append(total_score)
#       if(len(word_tokens)>0):
#         if(word_tokens[len(word_tokens)-1]+"_positive" in NRCwordDict):
#           Nrc_last_score.append(NRCwordDict.get(word_tokens[len(word_tokens)-1]+"_positive"))
#         elif(word_tokens[len(word_tokens)-1]+"_negative" in NRCwordDict):
#           Nrc_last_score.append(NRCwordDict.get(word_tokens[len(word_tokens)-1]+"_negative"))
#         else:
#           Nrc_last_score.append(0)    
#       else:
#         Nrc_last_score.append(0) 
#   features = pd.DataFrame()      
#   features['NRC lexicon greater than 0']=Nrc_tot_con_greaterthan0
#   features['NRC lexicon total score']=Nrc_tot_score
#   features['NRC lexicon last token']=Nrc_last_score
#   return features

# # Number of unimportant words
# def unimp_words(tweets,feature):
#   unimp=[]
#   for row in tweets:
#     tokens = row.split()
#     c=0
#     for word in tokens:
#        if (len(word)<5 or word in stopwords.words('english')):
#           c=c+1
#     unimp.append(c)      
#   # print(unimp)
#   # print(len(unimp))
#   feature['unimp words'] = unimp
#   return feature

# # Number of words per sentence
# def avg_word_persent(tweets,feature):
#   avg_len=[]
#   for row in tweets:
#     len_each_sen = []
#     lines = row.split(".")
#     for line in lines:
#       if(len(line)>4):
#           len_each_sen.append(len(line))
#     if(len(len_each_sen)!=0):      
#         avg = sum(len_each_sen)
#     else:
#         avg = 0
#     avg_len.append(avg)
#   # print(avg_len)
#   # print(len(avg_len))
#   feature['number of words'] = avg_len
#   return feature

# #finding unique tokens count in each news article.
# def lexical_diversity(news,features):
#   l=[]
#   for n in news:
#     tokens=nltk.word_tokenize(n)
#     l.append(float(len(set(tokens))))
#   features['lex_diversity']=l
#   return features

# features=pd.DataFrame()
# test_features = pd.DataFrame()
# train_df = pd.read_csv("/content/drive/My Drive/NLP Project/train.csv")
# test_df = pd.read_csv("/content/drive/MyDrive/NLP Project/test.csv")
# features=polarWordCount(wordDict,train_df['text'],features)
# test_features = polarWordCount(wordDict,test_df['text'],test_features)
# features=polarityScore(train_df['text'],features)
# test_features=polarityScore(test_df['text'],test_features)
# print(features)
# print(test_features)
# features = lexiconFeature(train_df['text'].astype(str)).astype(float)
# test_feature = lexiconFeature(test_df['text'].astype(str)).astype(float)
# print(features)
# print(test_feature)
# features = unimp_words(train_df['text'].astype(str),features)   
# test_feature = unimp_words(test_df['text'].astype(str),test_feature) 
# features = avg_word_persent(train_df['text'].astype(str),features) 
# test_feature = avg_word_persent(test_df['text'].astype(str),test_feature)
# features = lexical_diversity(train_df['text'].astype(str),features)
# print(features)

# newData.to_csv('/content/drive/MyDrive/NLP Project/finalPrefeatTrain.csv')
# newDatat.to_csv('/content/drive/MyDrive/NLP Project/finalPrefeatTest.csv')

# tr= pd.read_csv("/content/drive/MyDrive/NLP Project/finalPrefeatTrain.csv")
# te=   pd.read_csv("/content/drive/MyDrive/NLP Project/finalPrefeatTest.csv")
# features=pd.read_csv("/content/drive/MyDrive/NLP Project/sakshi_features(29.11.2020).csv")
# featuresTest= pd.read_csv("/content/drive/MyDrive/NLP Project/sakshi_features_test(29.11.2020).csv")
# ftr=pd.read_csv("/content/drive/MyDrive/NLP Project/sakshi_train_features(30.11.2020).csv")
# fte=pd.read_csv("/content/drive/MyDrive/NLP Project/sakshi_test_features(30.11.2020).csv")

# frame1=[tr,features,ftr]
# frame2=[te,featuresTest,fte]
# trr = pd.concat(frame1,axis=1)
# tee = pd.concat(frame2,axis=1)

import itertools
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer

tfidf = TfidfVectorizer(stop_words = 'english', max_df = 0.7,max_features=1000)
tfidf_train = tfidf.fit_transform(trr['preCleantext']) 
tfidf_test = tfidf.transform(tee['preCleantext'].values.astype('U'))


df_train = pd.DataFrame(tfidf_train.toarray(), columns= tfidf.get_feature_names())
trr= pd.concat([trr,df_train], axis=1,ignore_index=True,join='inner')

df_test=pd.DataFrame(tfidf_test.toarray(), columns= tfidf.get_feature_names())
tee= pd.concat([tee,df_test], axis=1,ignore_index=True,join='inner')

tfidf.get_feature_names()[-10:]

trr.head()

trr.shape

label=trr['label']
labels=pd.DataFrame(label)
labels.to_csv('/content/drive/MyDrive/NLP Project/labelss.csv',index =False)

trr=pd.read_csv("/content/drive/MyDrive/NLP Project/FinalFeaturesTrain.csv")
tee=pd.read_csv("/content/drive/MyDrive/NLP Project/FinalFeaturesTest.csv")

frame=[trr,tee]
tot=pd.concat(frame,axis=0)
tot.shape

import seaborn as sn
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

lgb = LGBMClassifier(learning_rate= 0.5, n_estimators=800,max_depth=70,tree_method='gpu_hist',boosting_type='gbdt', objective='binary')
lgb.fit(final_train, label)
pred = lgb.predict(final_test)

pp = pred.astype(int)
idd=pd.read_csv("/content/drive/MyDrive/NLP Project/idd.csv")
fr = pd.DataFrame(columns=['id','label'])
fr['id'] = idd['id']
fr['label'] = pp.tolist()
fr.to_csv('OURlgbm4.csv', index=False)

rf = RandomForestClassifier(n_estimators=100)
rf.fit(trr, label)
pred_rf = rf.predict(tee)
pp = pred_rf.astype(int)
idd=pd.read_csv("/content/drive/MyDrive/NLP Project/idd.csv")
fr = pd.DataFrame(columns=['id','label'])
fr['id'] = idd['id']
fr['label'] = pp.tolist()
fr.to_csv('ourRF.csv', index=False)

mlp=MLPClassifier()
mlp.fit(trr, label)
pred_mlp = mlp.predict(tee)
pp = pred_mlp.astype(int)
idd=pd.read_csv("/content/drive/MyDrive/NLP Project/idd.csv")
fr = pd.DataFrame(columns=['id','label'])
fr['id'] = idd['id']
fr['label'] = pp.tolist()
fr.to_csv('ourMLP.csv', index=False)

dtr=DecisionTreeClassifier()
dtr.fit(trr, label)
pred_dtr = dtr.predict(tee)
pp = pred_dtr.astype(int)
idd=pd.read_csv("/content/drive/MyDrive/NLP Project/idd.csv")
fr = pd.DataFrame(columns=['id','label'])
fr['id'] = idd['id']
fr['label'] = pp.tolist()
fr.to_csv('ourDTR.csv', index=False)

sv=svm.SVC()
sv.fit(trr, label)
pred_sv = sv.predict(tee)
pp = pred_sv.astype(int)
idd=pd.read_csv("/content/drive/MyDrive/NLP Project/idd.csv")
fr = pd.DataFrame(columns=['id','label'])
fr['id'] = idd['id']
fr['label'] = pp.tolist()
fr.to_csv('ourSVC.csv', index=False)

final_train=pd.read_csv("/content/drive/MyDrive/NLP Project/FinalFeaturesTrain.csv")
final_test=pd.read_csv("/content/drive/MyDrive/NLP Project/FinalFeaturesTest.csv")

final_train.to_csv("/content/drive/MyDrive/NLP Project/SubmissionFeatureTrain.csv")
final_test.to_csv("/content/drive/MyDrive/NLP Project/SubmissionFeatureTest.csv")

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import StackingClassifier
estimators = [
               ('dtre',DecisionTreeClassifier(max_depth=70,criterion='entropy')),
               ('xgb',XGBClassifier(learning_rate=0.1,n_estimators=500,max_depth=9,min_child_weight=4,base_score=0.06,tree_method='gpu_hist')),
               ('lgb',LGBMClassifier(learning_rate= 0.05, n_estimators=500,max_depth=50,tree_method='gpu_hist',boosting_type='gbdt', objective='binary')),
               ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
               ('svr', svm.SVC(random_state=42))
               ]
clf = StackingClassifier(
    estimators=estimators, final_estimator= LogisticRegression()
 )

clf.fit(final_train,label)
pred_clf = clf.predict(final_test)
pp = pred_clf.astype(int)
idd=pd.read_csv("/content/drive/MyDrive/NLP Project/idd.csv")
fr = pd.DataFrame(columns=['id','label'])
fr['id'] = idd['id']
fr['label'] = pp.tolist()
fr.to_csv('ourStacked.csv', index=False)

xgb=XGBClassifier(learning_rate=0.1,n_estimators=500,max_depth=9,min_child_weight=4,base_score=0.06,tree_method='gpu_hist')
xgb.fit(final_train, label)
pred_xgb = xgb.predict(final_test)
pp = pred_xgb.astype(int)
idd=pd.read_csv("/content/drive/MyDrive/NLP Project/idd.csv")
fr = pd.DataFrame(columns=['id','label'])
fr['id'] = idd['id']
fr['label'] = pp.tolist()
fr.to_csv('ourXGB.csv', index=False)

lr=LogisticRegression(penalty='l2')
lr.fit(final_train, label)
pred_lr = lr.predict(final_test)
pp = pred_lr.astype(int)
idd=pd.read_csv("/content/drive/MyDrive/NLP Project/idd.csv")
fr = pd.DataFrame(columns=['id','label'])
fr['id'] = idd['id']
fr['label'] = pp.tolist()
fr.to_csv('ourLR.csv', index=False)

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(final_train, label)
pred_knn = knn.predict(final_test)
pp = pred_knn.astype(int)
idd=pd.read_csv("/content/drive/MyDrive/NLP Project/idd.csv")
fr = pd.DataFrame(columns=['id','label'])
fr['id'] = idd['id']
fr['label'] = pp.tolist()
fr.to_csv('ourKNN.csv', index=False)

"""TRAIN METRICS"""

x_train, x_test, y_train, y_test = train_test_split(final_train, labels, test_size=0.3, shuffle=True)
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix
import seaborn as sn

#logistic
lr = LogisticRegression()
lr.fit(x_train, y_train)
pred = lr.predict(x_test)
scoreLR = accuracy_score(y_test, pred)
print("accuracy:   %0.3f" % (scoreLR*100))
cm = confusion_matrix(y_test, pred, labels=[1, 0])
df_cm = pd.DataFrame(cm, range(2), range(2))
sn.set(font_scale=1)
sn.heatmap(df_cm, annot=True, annot_kws={'size':14}, fmt='d').set_title('Confusion Matrix LOGREG')

#random forest
rf = RandomForestClassifier(n_estimators=100)
rf.fit(x_train, y_train)
pred = rf.predict(x_test)
scoreRF = accuracy_score(y_test, pred)
print("accuracy:   %0.3f" % (scoreRF*100))
cm = confusion_matrix(y_test, pred, labels=[1, 0])
df_cm = pd.DataFrame(cm, range(2), range(2))
sn.set(font_scale=1)
sn.heatmap(df_cm, annot=True, annot_kws={'size':14}, fmt='d').set_title('Confusion Matrix RandomForest')

xgb = XGBClassifier(learning_rate=0.1,n_estimators=500,max_depth=9,min_child_weight=4,base_score=0.06,tree_method='gpu_hist') #96.692
xgb.fit(x_train, y_train)
pred = xgb.predict(x_test)
scoreXG = accuracy_score(y_test, pred)
print("accuracy:   %0.3f" % (scoreXG*100))
cm = confusion_matrix(y_test, pred, labels=[1, 0])
df_cm = pd.DataFrame(cm, range(2), range(2))
sn.set(font_scale=1)
sn.heatmap(df_cm, annot=True, annot_kws={'size':14}, fmt='d').set_title('Confusion Matrix XGB')

lgb = LGBMClassifier(learning_rate= 0.05, n_estimators=500,max_depth=30,tree_method='gpu_hist',  
    boosting_type='gbdt',
    objective='binary'
)
lgb.fit(x_train, y_train)
pred = lgb.predict(x_test)
scoreLGB= accuracy_score(y_test, pred)
print("accuracy:   %0.3f" % (scoreLGB *100))
cm = confusion_matrix(y_test, pred, labels=[1, 0])
df_cm = pd.DataFrame(cm, range(2), range(2))
sn.set(font_scale=1)
sn.heatmap(df_cm, annot=True, annot_kws={'size':14}, fmt='d').set_title('Confusion Matrix LGB')

mlp = MLPClassifier()
mlp.fit(x_train, y_train)
pred = mlp.predict(x_test)
scoreMLP= accuracy_score(y_test, pred)
print("accuracy:   %0.3f" % (scoreMLP*100))
cm = confusion_matrix(y_test, pred, labels=[1, 0])
df_cm = pd.DataFrame(cm, range(2), range(2))
sn.set(font_scale=1)
sn.heatmap(df_cm, annot=True, annot_kws={'size':14}, fmt='d').set_title('Confusion Matrix MLP')

dt = DecisionTreeClassifier(criterion='entropy',max_depth=  70)
dt.fit(x_train, y_train)
pred = dt.predict(x_test)
scored = accuracy_score(y_test, pred)
print("accuracy:   %0.3f" % (scored*100))
cm = confusion_matrix(y_test, pred, labels=[1, 0])
df_cm = pd.DataFrame(cm, range(2), range(2))
sn.set(font_scale=1)
sn.heatmap(df_cm, annot=True, annot_kws={'size':14}, fmt='d').set_title('Confusion Matrix DTREE')

sv = svm.SVC()
sv.fit(x_train, y_train)
pred = sv.predict(x_test)
scoreSVM = accuracy_score(y_test, pred)
print("accuracy:   %0.3f" % (scoreSVM*100))
cm = confusion_matrix(y_test, pred, labels=[1, 0])
df_cm = pd.DataFrame(cm, range(2), range(2))
sn.set(font_scale=1)
sn.heatmap(df_cm, annot=True, annot_kws={'size':14}, fmt='d').set_title('Confusion Matrix SVM')

knn=KNeighborsClassifier()
knn.fit(x_train, label)
pred_knn = knn.predict(x_test)
scoreKNN = accuracy_score(y_test, pred)
print("accuracy:   %0.3f" % (scoreKNN*100))
cm = confusion_matrix(y_test, pred, labels=[1, 0])
df_cm = pd.DataFrame(cm, range(2), range(2))
sn.set(font_scale=1)
sn.heatmap(df_cm, annot=True, annot_kws={'size':14}, fmt='d').set_title('Confusion Matrix KNN')

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import StackingClassifier
estimators = [
               ('dtre',DecisionTreeClassifier(max_depth=70,criterion='entropy')),
               ('xgb',XGBClassifier(learning_rate=0.1,n_estimators=500,max_depth=9,min_child_weight=4,base_score=0.06,tree_method='gpu_hist')),
               ('lgb',LGBMClassifier(learning_rate= 0.05, n_estimators=500,max_depth=50,tree_method='gpu_hist',boosting_type='gbdt', objective='binary')),
               ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
               ('svr', svm.SVC(random_state=42))
               ]
clf = StackingClassifier(
    estimators=estimators, final_estimator= LogisticRegression()
 )

clf.fit(x_train,y_train)
pred = clf.predict(x_test)
scoreStacked = accuracy_score(y_test, pred)
print("accuracy:   %0.3f" % (scoreStacked*100))
cm = confusion_matrix(y_test, pred, labels=[1, 0])
df_cm = pd.DataFrame(cm, range(2), range(2))
sn.set(font_scale=1)
sn.heatmap(df_cm, annot=True, annot_kws={'size':14}, fmt='d').set_title('Confusion Matrix stacked')

import plotly.graph_objects as go

fig = go.Figure(data=[go.Table(header=dict(values=['CLASSIFIER', 'SCORES']),
                 cells=dict(values=[["Logistic Regression","Random Forest","XG Boost","Light GBM","Decision Tree","Multi-LayerPerceptron","SVM","KNN","Stacked"],
                                    [scoreLR*100, scoreRF*100,scoreXG*100,scoreLGB*100, scored*100, scoreMLP*100, scoreSVM*100,scoreKNN*100,scoreStacked*100]]))
                     ])
tab=pd.DataFrame(fig)
fig.show()

import plotly.express as px
data = {'CLASSIFIER': ["Logistic Regression","Random Forest","XG Boost","Light GBM","Decision Tree","Multi-LayerPerceptron","SVM","KNN","STACKED"], 'SCORES': [scoreLR*100, scoreRF*100,scoreXG*100,scoreLGB*100, scored*100, scoreMLP*100, scoreSVM*100,scoreKNN*100,scoreStacked*100]}
tab=pd.DataFrame.from_dict(data)

fig = px.bar(tab, x='CLASSIFIER', y='SCORES', hover_data=['SCORES'], color='SCORES',  height=400)
fig.show()