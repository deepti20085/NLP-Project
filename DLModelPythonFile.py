# -*- coding: utf-8 -*-
"""Nlp_project(30.11.2020).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TaFe1MG8WfI_-WPi5dtiKbK1T0gmbURl
"""

from google.colab import drive
drive.mount("/content/drive")

!pip install contractions
!pip install simpletransformers

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, KFold
import re
from nltk.corpus import stopwords
from html.parser import HTMLParser
import contractions
from nltk.tokenize import word_tokenize
import nltk
from nltk.corpus import stopwords
from textblob import TextBlob
import sys
import numpy as np
from sklearn.model_selection import train_test_split
from simpletransformers.classification import ClassificationModel
import torch

# cleaning 
def preprocessing(df,flag):
  if (flag==1):
    # Eliminating null rows
    df=df.dropna(subset=['text','label'])
    #print(len(df))
    #removing duplicates
    df.drop_duplicates(subset=['text'],keep='first',inplace=True)
    #print(len(df))
  elif (flag==0):
    df.fillna('' ,inplace=True)
  text=df['text']
  #print(text)  
  
  clean_text=[]
  html_parser = HTMLParser()
  for sent in text:
  # for sent in train_df['5']:
   if(len(sent)>0):
     clean_S = html_parser.unescape(sent)
     #clean_S = contractions.fix(clean_S)   #expand
     clean_S = re.sub(r"@(\s+|\w+)","",clean_S)  #users eliminate
     clean_S = re.sub(r"https?:\/\/.*", "", clean_S)   #urls eliminate
     clean_S = re.sub(r"www.(\w+\.\w+)+", "", clean_S)   #urls eliminate
     clean_S = re.sub('[^a-zA-Z0-9!\?\s.,]', '', clean_S)  #to eliminate non ascii character
     clean_S = re.sub(r"\s{2,}[\.|,]+\s{2,}","",clean_S)
     clean_S = re.sub(r"[\.,]{2,}"," ",clean_S)
     clean_S=re.sub("[^a-zA-Z]", " ",clean_S) # removing punctuation and digits
     clean_S = clean_S.lower()
     clean_text.append(clean_S)
   else:
     clean_text.append("")  
  df['text']=clean_text

  return df

#train the model on training set.
def trainmodel(model_name,model_type,df,savemodel,steps):
  model=ClassificationModel(model_name, model_type, num_labels=2, args={'reprocess_input_data': True, 'overwrite_output_dir': True,'logginig_steps':steps})
  model.train_model(df)
  torch.save(model,savemodel)    
  return model

#validating the model using validation data.
def validatedata(filepath,df):
  saved_model=torch.load(filepath)
  res,val_pred,wrong_pred=saved_model.eval_model(df)
  pred_labels=[]
  for a in val_pred:
      pred_labels.append(np.argmax(a))
  return pred_labels

#confusion and classification report.
def evaluate(pred_labels,actual_labels): 
  import sklearn
  print(sklearn.metrics.confusion_matrix(actual_labels,pred_labels))
  print(sklearn.metrics.classification_report(actual_labels,pred_labels,target_names=['real','fake']))

#predicting the labels.
def predictlabels(df,filepath):
  saved_model=torch.load(filepath)
  print(saved_model)
  pred_test,output=saved_model.predict(df.text)
  return pred_test

#save data to csv file.
def saveToCSV(pred_test,filename):
  submit_df=pd.read_csv('/content/drive/My Drive/NLP Project/fake-news/submit.csv')
  submit_df['label']=pred_test
  submit_df.to_csv(filename,index=False)

def Stacking(df_train,y_train,df_test):
  model=LogisticRegression(random_state=1)
  model.fit(df_train,y_train)
  final_label=model.predict(df_test)
  return list(final_label)

train_df = pd.read_csv("/content/drive/My Drive/NLP Project/fake-news/train.csv")
test_df = pd.read_csv("/content/drive/My Drive/NLP Project/fake-news/test.csv")

train_df=preprocessing(train_df,1)
training_df,validate_df = train_test_split(train_df,test_size = 0.02)

filter_train_df=pd.DataFrame(columns=['text','label'])
filter_train_df['text']=training_df['text']
filter_train_df['label']=training_df['label']

filter_training_df=pd.DataFrame(columns=['text','label'])
filter_training_df['text']=train_df['text']
filter_training_df['label']=train_df['label']

bertmodel=trainmodel('bert', 'bert-base-cased',filter_train_df,'/content/drive/My Drive/NLP Project/fake-news/bertmodel(26.11.2020)',50)

robertmodel=trainmodel('roberta','roberta-base',filter_train_df,'/content/drive/My Drive/NLP Project/fake-news/robertmodel_dprocess(26.11.2020).h5',30)

distilbertmodel=trainmodel('distilbert','distilbert-base-cased',filter_train_df,'/content/drive/My Drive/NLP Project/fake-news/distilmodel(28.11.2020)(1).h5',50)

filter_validate_df=pd.DataFrame(columns=['text','label'])
filter_validate_df['text']=validate_df['text']
filter_validate_df['label']=validate_df['label']

print(filter_validate_df.text)

actual_labels=[]
actual_labels=validate_df['label']

val_pred_bert=validatedata('/content/drive/My Drive/NLP Project/fake-news/bertmodel(26.11.2020).h5',filter_validate_df)

evaluate(val_pred_bert,actual_labels)

val_pred_robert=validatedata('/content/drive/My Drive/NLP Project/fake-news/robertmodel_dprocess(26.11.2020).h5',filter_validate_df)
evaluate(val_pred_robert,actual_labels)

val_pred_distilbert=validatedata('/content/drive/My Drive/NLP Project/fake-news/distilmodel(28.11.2020)(1).h5',filter_validate_df)
evaluate(val_pred_distilbert,actual_labels)

test_df=preprocessing(test_df,0)

filter_test_df=pd.DataFrame(columns=['text'])
filter_test_df['text']=test_df['text']

pred_train=predictlabels(train_df,'/content/drive/My Drive/NLP Project/fake-news/bertmodel(26.11.2020).h5')

pred_train1=predictlabels(train_df,'/content/drive/My Drive/NLP Project/fake-news/distilmodel(28.11.2020)(1).h5')

pred_train2=predictlabels(train_df,'/content/drive/My Drive/NLP Project/fake-news/robertmodel_dprocess(26.11.2020).h5')

pred_test=predictlabels(filter_test_df,'/content/drive/My Drive/NLP Project/fake-news/bertmodel(26.11.2020).h5')
pred_test1=predictlabels(filter_test_df,'/content/drive/My Drive/NLP Project/fake-news/distilmodel(28.11.2020)(1).h5')

pred_test2=predictlabels(filter_test_df,'/content/drive/My Drive/NLP Project/fake-news/robertmodel_dprocess(26.11.2020).h5')

pred_test2_df=pd.DataFrame(pred_test2)
pred_train2_df=pd.DataFrame(pred_train2)
pred_train_df=pd.DataFrame(pred_train)
pred_train1_df=pd.DataFrame(pred_train1)
pred_test_df=pd.DataFrame(pred_test)
pred_test1_df=pd.DataFrame(pred_test1)
y_train=train_df['label']
df_test=pd.concat([pred_test_df,pred_test1_df,pred_test2_df],axis=1)
df_train=pd.concat([pred_train_df, pred_train1_df,pred_train2_df],axis=1)

saveToCSV(final_label,'stacked30.11.2020_final.csv')

saveToCSV(pred_test,'bert30.11.2020_final.csv')

saveToCSV(pred_test1,'distilbert30.11.2020_final.csv')

saveToCSV(pred_test2,'robert30.11.2020_final.csv')

print("Classification report on training data using bert:")
evaluate(pred_train,y_train)

print("Classification report on training data using distilbert:")
evaluate(pred_train1,y_train)

print("Classification report on training data using roberto:")
evaluate(pred_train2,y_train)

print("Classification report on training data using stacked model:")
evaluate(final_train_label,y_train)

from sklearn.linear_model import LogisticRegression
final_label=Stacking(df_train,y_train,df_test)

#final train labels on stacking
final_train_label=Stacking(df_train,y_train,df_train)

# Input Output System
def prediction_dlmodel(model):
  text = input("Please enter the news text : ")
  # load_model = pickle.load(open('final_model.sav', 'rb'))
  preprocessing(text,0)
  prediction = model.predict([text])
  if(prediction[0]==0):
    print("Real")
  else
    print("Fake")
  # print(" 1. means real and 0 means hoax")
  # print(prediction[0])

prediction_dlmodel(robertmodel)